import logging
import os
from typing import Dict, Any, Optional, List, Callable

from openai import AsyncOpenAI

from tools.utils import emit_ws_message

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LLMClient:
    """
    é€šç”¨LLMè°ƒç”¨å®¢æˆ·ç«¯ï¼Œæ”¯æŒå¤šç§æ¨¡å‹å’Œæµå¼/éæµå¼å“åº”
    """
    
    def __init__(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.base_url = os.getenv("BASE_URL")
        self.inf_api_key = os.getenv("INF_API_KEY")
        self.default_model = os.getenv("MODEL", "gpt-4o-mini")
        
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY not found in environment variables")
            
        self.client = AsyncOpenAI(api_key=self.inf_api_key, base_url=self.base_url)
    
    async def call_llm(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: float = 0.3,
        max_tokens: int = 20000,
        stream: bool = True,
        websocket: Optional[Any] = None,
        response_handler: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨LLMç”Ÿæˆå“åº”
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            model: ä½¿ç”¨çš„æ¨¡å‹
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            stream: æ˜¯å¦æµå¼å“åº”
            websocket: WebSocketè¿æ¥ç”¨äºæµå¼ä¼ è¾“
            response_handler: å“åº”å¤„ç†å‡½æ•°
            
        Returns:
            åŒ…å«å“åº”å†…å®¹å’Œå…ƒæ•°æ®çš„å­—å…¸
        """
        try:
            # ä½¿ç”¨æä¾›çš„æ¨¡å‹æˆ–å›é€€åˆ°é»˜è®¤æ¨¡å‹
            model_to_use = model or self.default_model
            
            # å‘é€çŠ¶æ€æ¶ˆæ¯
            websocket_active = await emit_ws_message(websocket, "status", "ğŸ¤– æ­£åœ¨è°ƒç”¨AIæ¨¡å‹...")
            
            print(f"Debug - About to make API call")
            print(f"Debug - Model: {model_to_use}")
            print(f"Debug - Messages count: {len(messages)}")
            print(f"Debug - Temperature: {temperature}")
            print(f"Debug - Max tokens: {max_tokens}")
            print(f"Debug - Stream: {stream}")
            
            # è°ƒç”¨LLM
            response = await self.client.chat.completions.create(
                model=model_to_use,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=stream,
                extra_headers={'apikey': self.api_key} if self.api_key else None,
            )
            
            print("Debug - API call initiated successfully")
            
            # æ”¶é›†å“åº”
            response_content = ""
            if stream:
                # å¤„ç†æµå¼å“åº”
                if websocket_active:
                    websocket_active = await emit_ws_message(websocket, "stream_start", "å¼€å§‹ç”Ÿæˆ...")
                print("ğŸ“ Response:")
                print("-" * 50)
                
                async for chunk in response:
                    if len(chunk.choices) > 0:
                        if chunk.choices[0].delta.content is not None:
                            content = chunk.choices[0].delta.content
                            print(content, end="", flush=True)
                            response_content += content
                            # åªæœ‰WebSocketä»ç„¶æ´»è·ƒæ—¶æ‰å‘é€å—
                            if websocket_active:
                                websocket_active = await emit_ws_message(websocket, "chunk", content)
                
                print("\n" + "-" * 50)
                print("âœ… Generation complete!\n")
                if websocket_active:
                    await emit_ws_message(websocket, "status", "âœ… ç”Ÿæˆå®Œæˆ!")
            else:
                # å¤„ç†éæµå¼å“åº”
                response_content = response.choices[0].message.content
                print("ğŸ“ Response:")
                print("-" * 50)
                print(response_content)
                print("-" * 50)
                print("âœ… Generation complete!\n")
                
                # å¦‚æœWebSocketæ´»è·ƒï¼Œåˆ™å‘é€æ•´ä¸ªå“åº”
                if websocket_active:
                    await emit_ws_message(websocket, "stream_start", "å¼€å§‹ç”Ÿæˆ...")
                    await emit_ws_message(websocket, "chunk", response_content)
                    await emit_ws_message(websocket, "status", "âœ… ç”Ÿæˆå®Œæˆ!")
            
            # å¦‚æœæä¾›äº†å“åº”å¤„ç†å‡½æ•°ï¼Œåˆ™ä½¿ç”¨å®ƒå¤„ç†å“åº”
            if response_handler:
                return await response_handler(response_content)
            
            return {
                "success": True,
                "content": response_content
            }
            
        except Exception as e:
            error_msg = f"LLMè°ƒç”¨å¤±è´¥: {str(e)}"
            print(f"Debug - API call failed with error: {str(e)}")
            
            # å‘é€é”™è¯¯æ¶ˆæ¯
            await emit_ws_message(websocket, "error", error_msg)
            
            return {
                "success": False,
                "error": error_msg
            }